{"cells":[{"cell_type":"code","source":["#INPUT: train.csv\n#Output: \n\"\"\"\n\n1. Clean up wrong X and Y values (very few of them)\n\n2. visualize data.\n\n2. Parse input to get features: for e.g: get date, time, year, month, etc..)\n\n3. Select, and generate features.\n\n3. Remove outliers.\n\n4. do PCA\n\nOutput: train dataframe with features and labels column\n\n        test dataframe with features and lables column\n\n        visuals to provide insights on data that help select, and tune the models.       \n\n a toolbox list to choose from:\n\n         Typical graphical techniques used in EDA are\n\n\nBox plot\n\nHistogram\n\nMulti-vari chart\n\nRun chart\n\nPareto chart\n\nScatter plot\n\nStem-and-leaf plot\n\nParallel coordinates\n\nOdds ratio\n\nTargeted projection pursuit\n\nGlyph-based visualization methods such as PhenoPlot[8] and Chernoff faces\n\nProjection methods such as grand tour, guided tour and manual tour\n\nInteractive versions of these plots\n\n        Dimensionality reduction:\n\nMultidimensional scaling\n\nPrincipal component analysis (PCA)\n\nMultilinear PCA\n\nNonlinear dimensionality reduction (NLDR)\n\n        Typical quantitative techniques are:\nMedian polish\n\nTrimean\n\nOrdination\n\nHistory\n\n        \n\"\"\"\n\"\"\"\nDates - timestamp of the crime incident\nCategory - category of the crime incident (only in train.csv). This is the target variable you are going to predict.\nDescript - detailed description of the crime incident (only in train.csv)\nDayOfWeek - the day of the week\nPdDistrict - name of the Police Department District\nResolution - how the crime incident was resolved (only in train.csv)\nAddress - the approximate street address of the crime incident \nX - Longitude\nY - Latitude\n\"\"\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[1]: &#39;\\nDates - timestamp of the crime incident\\nCategory - category of the crime incident (only in train.csv). This is the target variable you are going to predict.\\nDescript - detailed description of the crime incident (only in train.csv)\\nDayOfWeek - the day of the week\\nPdDistrict - name of the Police Department District\\nResolution - how the crime incident was resolved (only in train.csv)\\nAddress - the approximate street address of the crime incident \\nX - Longitude\\nY - Latitude\\n&#39;</div>"]}}],"execution_count":1},{"cell_type":"code","source":["filename=\"/FileStore/tables/train.csv\"\ndata=spark.read.csv(filename, header=True, inferSchema=True)\nprint(data.count())\nprint(len(data.columns))\ndata.printSchema() #the data was inferred properly. Class is an int. Features are double."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">878049\n9\nroot\n-- Dates: timestamp (nullable = true)\n-- Category: string (nullable = true)\n-- Descript: string (nullable = true)\n-- DayOfWeek: string (nullable = true)\n-- PdDistrict: string (nullable = true)\n-- Resolution: string (nullable = true)\n-- Address: string (nullable = true)\n-- X: double (nullable = true)\n-- Y: double (nullable = true)\n\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["data = data.drop('Descript').drop('Resolution') # we drop both as they are not available in test data. "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["#1. Data Wrangling to audit the quality of the data and perform all the necessary actions to clean the dataset.\n#1- check how many categorical and numerical features we have\ncat_cols = [item[0] for item in data.dtypes if (item[1]=='string') & (item[0]!='Category')]\nprint(str(len(cat_cols)) + '  categorical features')\nnum_var = [i[0] for i in data.dtypes if ((i[1]=='int') | (i[1]=='double')) ]\nprint(str(len(num_var)) + '  numerical features')\n#Last feature is timestamp"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">3  categorical features\n2  numerical features\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["#class count (labels)\nprint('Number of labels is', data.groupBy('Category').count().count())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Number of labels is 39\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["labels=data[['Category']].distinct().toPandas().values.tolist()\nlabels #we shall use it later.\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/python/lib/python3.7/site-packages/pyarrow/__init__.py:152: UserWarning: pyarrow.open_stream is deprecated, please use pyarrow.ipc.open_stream\n  warnings.warn(&#34;pyarrow.open_stream is deprecated, please use &#34;\nOut[6]: [[&#39;FRAUD&#39;],\n [&#39;SUICIDE&#39;],\n [&#39;SEX OFFENSES FORCIBLE&#39;],\n [&#39;LIQUOR LAWS&#39;],\n [&#39;SECONDARY CODES&#39;],\n [&#39;FAMILY OFFENSES&#39;],\n [&#39;MISSING PERSON&#39;],\n [&#39;OTHER OFFENSES&#39;],\n [&#39;DRIVING UNDER THE INFLUENCE&#39;],\n [&#39;WARRANTS&#39;],\n [&#39;ARSON&#39;],\n [&#39;SEX OFFENSES NON FORCIBLE&#39;],\n [&#39;FORGERY/COUNTERFEITING&#39;],\n [&#39;GAMBLING&#39;],\n [&#39;BRIBERY&#39;],\n [&#39;ASSAULT&#39;],\n [&#39;DRUNKENNESS&#39;],\n [&#39;EXTORTION&#39;],\n [&#39;TREA&#39;],\n [&#39;WEAPON LAWS&#39;],\n [&#39;LOITERING&#39;],\n [&#39;SUSPICIOUS OCC&#39;],\n [&#39;ROBBERY&#39;],\n [&#39;PROSTITUTION&#39;],\n [&#39;EMBEZZLEMENT&#39;],\n [&#39;BAD CHECKS&#39;],\n [&#39;DISORDERLY CONDUCT&#39;],\n [&#39;RUNAWAY&#39;],\n [&#39;RECOVERED VEHICLE&#39;],\n [&#39;VANDALISM&#39;],\n [&#39;DRUG/NARCOTIC&#39;],\n [&#39;PORNOGRAPHY/OBSCENE MAT&#39;],\n [&#39;TRESPASS&#39;],\n [&#39;VEHICLE THEFT&#39;],\n [&#39;NON-CRIMINAL&#39;],\n [&#39;STOLEN PROPERTY&#39;],\n [&#39;LARCENY/THEFT&#39;],\n [&#39;KIDNAPPING&#39;],\n [&#39;BURGLARY&#39;]]</div>"]}}],"execution_count":6},{"cell_type":"code","source":["#check for nulls\nfrom pyspark.sql.functions import isnan, when, count, col\ndata.select([count(when( col(c).isNull(), c)).alias(c) for c in data.columns]).show()\n#conclusion :From above it seems the data is clean with no missing values"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+--------+---------+----------+-------+---+---+\nDates|Category|DayOfWeek|PdDistrict|Address|  X|  Y|\n+-----+--------+---------+----------+-------+---+---+\n    0|       0|        0|         0|      0|  0|  0|\n+-----+--------+---------+----------+-------+---+---+\n\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["#check for data imbalance (classes imbalance)\n#report distinct classes and their prior probabilities as in the dataset.\n\nclassList = data.select('Category').groupBy('Category').agg((100*count('Category')/data.count()).alias('prior'))\nprint(classList.toPandas())\n\n#consider removing 'Other offences', note that it accounts for 14%"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">                       Category      prior\n0                         FRAUD   1.899552\n1                       SUICIDE   0.057856\n2         SEX OFFENSES FORCIBLE   0.499744\n3                   LIQUOR LAWS   0.216731\n4               SECONDARY CODES   1.137180\n5               FAMILY OFFENSES   0.055919\n6                MISSING PERSON   2.959858\n7                OTHER OFFENSES  14.370724\n8   DRIVING UNDER THE INFLUENCE   0.258300\n9                      WARRANTS   4.807704\n10                        ARSON   0.172314\n11    SEX OFFENSES NON FORCIBLE   0.016856\n12       FORGERY/COUNTERFEITING   1.208247\n13                     GAMBLING   0.016628\n14                      BRIBERY   0.032914\n15                      ASSAULT   8.755320\n16                  DRUNKENNESS   0.487444\n17                    EXTORTION   0.029156\n18                         TREA   0.000683\n19                  WEAPON LAWS   0.974319\n20                    LOITERING   0.139514\n21               SUSPICIOUS OCC   3.577705\n22                      ROBBERY   2.619444\n23                 PROSTITUTION   0.852344\n24                 EMBEZZLEMENT   0.132794\n25                   BAD CHECKS   0.046239\n26           DISORDERLY CONDUCT   0.492000\n27                      RUNAWAY   0.221628\n28            RECOVERED VEHICLE   0.357383\n29                    VANDALISM   5.093679\n30                DRUG/NARCOTIC   6.146696\n31      PORNOGRAPHY/OBSCENE MAT   0.002506\n32                     TRESPASS   0.834350\n33                VEHICLE THEFT   6.125057\n34                 NON-CRIMINAL  10.512397\n35              STOLEN PROPERTY   0.517055\n36                LARCENY/THEFT  19.919162\n37                   KIDNAPPING   0.266614\n38                     BURGLARY   4.185985\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["#explore categorical features\n#check # of unique number of categories for categorical features.\nif 'Category' not in cat_cols:\n  cat_cols.append('Category') #add category to cat_cols to view unique values of labels and for the remaining steps\ncountUniqueValues = [data.select(c).distinct().count() for c in cat_cols]\nprint(cat_cols)\nprint(countUniqueValues)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&#39;DayOfWeek&#39;, &#39;PdDistrict&#39;, &#39;Address&#39;, &#39;Category&#39;]\n[7, 10, 23228, 39]\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["#identify the most frequent items in the categorical features.\nfrom pyspark.sql.functions import desc\nN=10 #the N number of most frequent points.\nfrequentCategories = [ data.groupBy(c).agg((100*count(c)/data.count()).alias('Percentage')).sort(desc('Percentage')).limit(N) for c in cat_cols]\nfor i in range(len(cat_cols)):\n  print(frequentCategories[i].toPandas())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">   DayOfWeek  Percentage\n0     Friday   15.230813\n1  Wednesday   14.715694\n2   Saturday   14.442246\n3   Thursday   14.240435\n4    Tuesday   14.232121\n5     Monday   13.847063\n6     Sunday   13.291627\n   PdDistrict  Percentage\n0    SOUTHERN   17.901279\n1     MISSION   13.656185\n2    NORTHERN   11.992041\n3     BAYVIEW   10.185195\n4     CENTRAL    9.732942\n5  TENDERLOIN    9.317134\n6   INGLESIDE    8.979567\n7     TARAVAL    7.470654\n8        PARK    5.616201\n9    RICHMOND    5.148801\n                    Address  Percentage\n0    800 Block of BRYANT ST    3.021813\n1    800 Block of MARKET ST    0.749503\n2  2000 Block of MISSION ST    0.580492\n3  1000 Block of POTRERO AV    0.462730\n4    900 Block of MARKET ST    0.370253\n5        0 Block of TURK ST    0.367633\n6         0 Block of 6TH ST    0.328455\n7     300 Block of ELLIS ST    0.307842\n8     400 Block of ELLIS ST    0.294972\n9      16TH ST / MISSION ST    0.285178\n         Category  Percentage\n0   LARCENY/THEFT   19.919162\n1  OTHER OFFENSES   14.370724\n2    NON-CRIMINAL   10.512397\n3         ASSAULT    8.755320\n4   DRUG/NARCOTIC    6.146696\n5   VEHICLE THEFT    6.125057\n6       VANDALISM    5.093679\n7        WARRANTS    4.807704\n8        BURGLARY    4.185985\n9  SUSPICIOUS OCC    3.577705\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["#check correlations of features with label column (crime category)\nN=10 #the N number of most frequent pairs of features and class.\nif 'Category' in cat_cols:\n  cat_cols.remove('Category') #remove Category from cat_cols to validate syntax in the loop\nfrequentCategories = [data.groupBy(c, 'Category').agg((100*count(c)/data.count()).alias('PairFrequencyPercentage')).sort(desc('PairFrequencyPercentage')).limit(N) for c in cat_cols]\nfor i in range(len(cat_cols)):\n  print(frequentCategories[i].toPandas())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">   DayOfWeek        Category  PairFrequencyPercentage\n0   Saturday   LARCENY/THEFT                 3.099713\n1     Friday   LARCENY/THEFT                 3.086844\n2  Wednesday   LARCENY/THEFT                 2.788797\n3   Thursday   LARCENY/THEFT                 2.780597\n4     Sunday   LARCENY/THEFT                 2.750416\n5    Tuesday   LARCENY/THEFT                 2.728435\n6     Monday   LARCENY/THEFT                 2.684360\n7  Wednesday  OTHER OFFENSES                 2.272083\n8    Tuesday  OTHER OFFENSES                 2.142136\n9     Friday  OTHER OFFENSES                 2.116966\n   PdDistrict        Category  PairFrequencyPercentage\n0    SOUTHERN   LARCENY/THEFT                 4.765679\n1    NORTHERN   LARCENY/THEFT                 3.260638\n2     CENTRAL   LARCENY/THEFT                 2.854055\n3    SOUTHERN  OTHER OFFENSES                 2.426744\n4    SOUTHERN    NON-CRIMINAL                 2.248736\n5     MISSION  OTHER OFFENSES                 2.201472\n6     MISSION   LARCENY/THEFT                 2.075397\n7  TENDERLOIN   DRUG/NARCOTIC                 2.015377\n8     BAYVIEW  OTHER OFFENSES                 1.942147\n9  TENDERLOIN  OTHER OFFENSES                 1.563011\n                    Address        Category  PairFrequencyPercentage\n0    800 Block of BRYANT ST   LARCENY/THEFT                 0.699733\n1    800 Block of BRYANT ST    NON-CRIMINAL                 0.635842\n2    800 Block of MARKET ST   LARCENY/THEFT                 0.370822\n3    800 Block of BRYANT ST  OTHER OFFENSES                 0.343830\n4    800 Block of BRYANT ST         ASSAULT                 0.219350\n5  2000 Block of MISSION ST   DRUG/NARCOTIC                 0.212517\n6    800 Block of BRYANT ST        WARRANTS                 0.195775\n7   1400 Block of PHELPS ST  MISSING PERSON                 0.167189\n8  2000 Block of MISSION ST  OTHER OFFENSES                 0.137692\n9    800 Block of BRYANT ST       VANDALISM                 0.131542\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["#Explore numerical features. "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"code","source":["countUniqueValuesN = [data.select(c).distinct().count() for c in num_var]\nprint(num_var)\nprint(countUniqueValuesN)\n#X, Y repeat.. possibly same locations witness several crimes over and over!"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&#39;X&#39;, &#39;Y&#39;]\n[34243, 34243]\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["#to confirm that X repeats when Y repeat\nNcrimeLocations = data.groupBy('X', 'Y').agg(count('X')).count()\nprint(NcrimeLocations) #shows only 34243 unique pairs"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">34243\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["#we will need to visualize the data\nimport matplotlib.pyplot as plt\nimport pandas as pd\nN1=data.count() if data.count()<=1000000 else 1000000"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":15},{"cell_type":"code","source":["#plot a historgram of timestamp to check if a pattern is there.\nimport numpy as np\ndata_pd_timestamp = data[['Dates']].limit(N1).toPandas()\nplt.clf()\nplt.hist(np.array(data_pd_timestamp['Dates']), bins = 100)\ndisplay(plt.show())"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# check for outliers\n#check in XY spatial data to spot outliers. \n#visualize data to spot outliers.\n\ndata_pd = data[['X', 'Y']].limit(N1).toPandas()\nplt.clf()\ndata_pd.plot(kind=\"scatter\", x=\"X\", y=\"Y\")\ndisplay(plt.show())"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["#Outliers can be seen. Dropping them."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":18},{"cell_type":"code","source":["outliersCount=data.count()\ndata=data.where('X<-122')\noutliersCount-=data.count()\ndata_pd=data.toPandas()\nplt.clf()\ndata_pd.plot(kind=\"scatter\", x=\"X\", y=\"Y\")\ndisplay(plt.show())"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["print('Removing ', str(outliersCount), ' outliers in spatial data.') #67 cases."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Removing  67  outliers in spatial data.\n</div>"]}}],"execution_count":20},{"cell_type":"code","source":["#plot XY data for different categories. \nNX=100\nNY=100\n\ngroups = [(data.where(col('Category')==crime[0]).toPandas(), crime[0])  for crime in labels] #used pandas df for visualization..\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":21},{"cell_type":"code","source":["#histogram values of x and y into bins and dipslay the bins content using imshow(). the light colors indicate high number of incidences. \nNX=70 #I find 70 to give good visualization. \nNY=70\ni=1\nfig= plt.figure(figsize=(30, 30)) #width and height in inches.\nfor g in groups:\n    group=g[0]\n    name=g[1]\n    plt.subplot(8,5,i)\n    crimeRegionsHistogram, xedges, yedges = np.histogram2d(np.array(group.X),np.array(group.Y), bins=(NX,NY)) # [int, int], the number of bins in each dimension (nx, ny = bins).\n    #xedges now contains the bin boundaries along x and so is yedges along y, the first and last boundary define the region (interval)\n    histoExtent  = [xedges[0],xedges[-1],yedges[0],yedges[-1]]\n    plt.imshow(crimeRegionsHistogram.T, extent=histoExtent, aspect='auto', origin='low') #the points are automatically scaled linearly mapping the lowest value to 0 and the highest to 1. \n    #plt.imshow(crimeRegionsHistogram.T,origin='low',extent=histoExtent,interpolation='nearest',aspect='auto') #transpose so each row list bins with common y range.\n    plt.title(name)\n    i+=1\ndisplay(plt.show())\n# we can tell how important is location features especially for some categories of crimes where the crime category is so dependent on location."],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["#the following code only plots the scatter plot of XY for different categories. You can tell that some crimes take place only in a short list of places. \ni=0\nj=0\nfig, axs = plt.subplots(8, 5, figsize=(30, 30))\nfor g in groups:\n    group=g[0]\n    name=g[1]\n    axs[i,j].scatter(group.X, group.Y)\n    axs[i,j].title.set_text(name)\n    if j < 4:\n      j+=1\n    else:\n      j=0\n      i+=1\n    \ndisplay(plt.show())"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["#SECOND BLOCK. PARSING AND FEATURE GENERATION."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":24},{"cell_type":"code","source":["#Parsing the time column to generate features, year, month, day, hour, season\nfrom pyspark.sql.functions import col, hour, minute, second, year, month, dayofmonth, date_format\n\ndef season(month):\n  switcher={\n    1:'winter',\n    2:'winter',\n    3:'spring',\n    4:'spring',\n    5:'spring',\n    6:'summer',\n    7:'summer',\n    8:'summer',\n    9:'autumn',\n    10:'autumn',\n    11:'autumn',\n    12:'winter'\n  }\n  return switcher.get(month,\"NA\")\n\n\nfrom pyspark.sql.types import StringType\nseason_udf_string= udf(lambda x: season(x), StringType())\n\ndata = data.withColumn(\"hour\", hour(col(\"Dates\"))).withColumn(\"minute\", minute(col(\"Dates\"))).withColumn(\"dayOfMonth\", dayofmonth(col(\"Dates\"))).withColumn(\"year\", year(col(\"Dates\"))).withColumn(\"month\", month(col(\"Dates\"))).withColumn(\"weekday\", date_format(col(\"Dates\"), \"EEEE\")).withColumn(\"season\", season_udf_string(col(\"month\"))).drop(col(\"Dates\"))\n\ndata.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- Category: string (nullable = true)\n-- DayOfWeek: string (nullable = true)\n-- PdDistrict: string (nullable = true)\n-- Address: string (nullable = true)\n-- X: double (nullable = true)\n-- Y: double (nullable = true)\n-- hour: integer (nullable = true)\n-- minute: integer (nullable = true)\n-- dayOfMonth: integer (nullable = true)\n-- year: integer (nullable = true)\n-- month: integer (nullable = true)\n-- weekday: string (nullable = true)\n-- season: string (nullable = true)\n\n</div>"]}}],"execution_count":25},{"cell_type":"code","source":["#generate a feature to tell whether day or night\n#data.select('hour').agg({\"hour\":\"max\"}).collect()[0]\n#data.select('hour').agg({\"hour\":\"min\"}).collect()[0]\ndef DayOrNight(h):\n  return 'Day' if (h<=18) & (h>=6) else 'Night'\nDayOrNight_udf_string= udf(lambda x: DayOrNight(x), StringType())\ndata = data.withColumn(\"DayOrNight\", DayOrNight_udf_string(col('hour')))\n#data.where(\"DayOrNight=='Day'\").count() \n#crimes occur more at day"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":26},{"cell_type":"code","source":["#generate an address feature:\naddress_df=data.select('Address')\naddress_df.show(100,False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------------------+\nAddress                      |\n+-----------------------------+\nOAK ST / LAGUNA ST           |\nOAK ST / LAGUNA ST           |\nVANNESS AV / GREENWICH ST    |\n1500 Block of LOMBARD ST     |\n100 Block of BRODERICK ST    |\n0 Block of TEDDY AV          |\nAVALON AV / PERU AV          |\nKIRKWOOD AV / DONAHUE ST     |\n600 Block of 47TH AV         |\nJEFFERSON ST / LEAVENWORTH ST|\nJEFFERSON ST / LEAVENWORTH ST|\n0 Block of ESCOLTA WY        |\nTURK ST / JONES ST           |\nFILLMORE ST / GEARY BL       |\n200 Block of WILLIAMS AV     |\n0 Block of MENDELL ST        |\nEDDY ST / JONES ST           |\nGODEUS ST / MISSION ST       |\nMENDELL ST / HUDSON AV       |\n100 Block of JONES ST        |\n200 Block of EVELYN WY       |\n1600 Block of VALENCIA ST    |\n100 Block of JONES ST        |\n100 Block of JONES ST        |\nFILLMORE ST / LOMBARD ST     |\n300 Block of OFARRELL ST     |\n2000 Block of BUSH ST        |\n500 Block of COLLEGE AV      |\n19TH AV / SANTIAGO ST        |\n2000 Block of 41ST AV        |\n1300 Block of WEBSTER ST     |\n400 Block of CASTRO ST       |\n1500 Block of FILLMORE ST    |\n1600 Block of WEBSTER ST     |\n1600 Block of WEBSTER ST     |\nKING ST / 3RD ST             |\nVALLEJO ST / BUCHANAN ST     |\nCALIFORNIA ST / BUCHANAN ST  |\n1400 Block of HOLLOWAY AV    |\n0 Block of WINDING WY        |\n700 Block of MARKET ST       |\n1600 Block of MARKET ST      |\n0 Block of STOCKTON ST       |\n300 Block of WILLIAMS AV     |\n300 Block of WILLIAMS AV     |\n800 Block of LEAVENWORTH ST  |\n0 Block of CRESCENT AV       |\nLINCOLN WY / 14TH AV         |\nSUTTER ST / POWELL ST        |\n1500 Block of HAIGHT ST      |\n23RD ST / WISCONSIN ST       |\n23RD ST / WISCONSIN ST       |\n0 Block of 3RD ST            |\n500 Block of BRANNAN ST      |\nOTIS ST / GOUGH ST           |\nMISSION ST / 2ND ST          |\nGEARY ST / VANNESS AV        |\nHARRISON ST / 10TH ST        |\nGEARY BL / LAGUNA ST         |\n0 Block of SANSOME ST        |\n0 Block of SANSOME ST        |\n900 Block of BUSH ST         |\n900 Block of BUSH ST         |\n0 Block of MARKET ST         |\n300 Block of 10TH ST         |\n19TH AV / WINSTON DR         |\n400 Block of HYDE ST         |\n400 Block of HYDE ST         |\n400 Block of HYDE ST         |\n400 Block of BRANNAN ST      |\n400 Block of BRANNAN ST      |\n2000 Block of SUTTER ST      |\n100 Block of PAUL AV         |\n26TH ST / GUERRERO ST        |\nSTOCKTON ST / SUTTER ST      |\n1900 Block of MISSION ST     |\n1900 Block of MISSION ST     |\n4600 Block of 18TH ST        |\n0 Block of FREELON ST        |\n17TH ST / TREAT AV           |\nPACIFIC AV / SANSOME ST      |\n1900 Block of POLK ST        |\n1900 Block of POLK ST        |\n200 Block of GOLDEN GATE AV  |\n3100 Block of HARRISON ST    |\n1000 Block of SCOTT ST       |\n1000 Block of POINTLOBOS AV  |\n1300 Block of FELTON ST      |\n1500 Block of FILLMORE ST    |\nPINE ST / GRANT AV           |\n600 Block of VANNESS AV      |\n400 Block of EDDY ST         |\nWESTPORTAL AV / VICENTE ST   |\nGREENWICH ST / LEAVENWORTH ST|\nBAY ST / HYDE ST             |\nSAN BRUNO AV / BURROWS ST    |\n25TH ST / SHOTWELL ST        |\n0 Block of 6TH ST            |\nILLINOIS ST / 20TH ST        |\nCASTRO ST / 16TH ST          |\n+-----------------------------+\nonly showing top 100 rows\n\n</div>"]}}],"execution_count":27},{"cell_type":"code","source":["from pyspark.sql.functions import split\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\ndata = data.withColumn(\"first\", split(\"Address\", \"/|of\").getItem(0)).withColumn(\"second\", split(\"Address\", \"/|of\").getItem(1)).withColumn(\"first\", trim(col(\"first\"))).withColumn(\"second\", trim(col(\"second\")))\ndata.show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------+---------+----------+--------------------+-------------------+------------------+----+------+----------+----+-----+---------+------+----------+----------+------------+\n      Category|DayOfWeek|PdDistrict|             Address|                  X|                 Y|hour|minute|dayOfMonth|year|month|  weekday|season|DayOrNight|     first|      second|\n+--------------+---------+----------+--------------------+-------------------+------------------+----+------+----------+----+-----+---------+------+----------+----------+------------+\n      WARRANTS|Wednesday|  NORTHERN|  OAK ST / LAGUNA ST|  -122.425891675136|  37.7745985956747|  23|    53|        13|2015|    5|Wednesday|spring|     Night|    OAK ST|   LAGUNA ST|\nOTHER OFFENSES|Wednesday|  NORTHERN|  OAK ST / LAGUNA ST|  -122.425891675136|  37.7745985956747|  23|    53|        13|2015|    5|Wednesday|spring|     Night|    OAK ST|   LAGUNA ST|\nOTHER OFFENSES|Wednesday|  NORTHERN|VANNESS AV / GREE...|   -122.42436302145|  37.8004143219856|  23|    33|        13|2015|    5|Wednesday|spring|     Night|VANNESS AV|GREENWICH ST|\n LARCENY/THEFT|Wednesday|  NORTHERN|1500 Block of LOM...|-122.42699532676599| 37.80087263276921|  23|    30|        13|2015|    5|Wednesday|spring|     Night|1500 Block|  LOMBARD ST|\n LARCENY/THEFT|Wednesday|      PARK|100 Block of BROD...|  -122.438737622757|37.771541172057795|  23|    30|        13|2015|    5|Wednesday|spring|     Night| 100 Block|BRODERICK ST|\n+--------------+---------+----------+--------------------+-------------------+------------------+----+------+----------+----+-----+---------+------+----------+----------+------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":28},{"cell_type":"code","source":["#Generate features:\n\n#define a pipeline to cluster X, Y data and generate a new feature; the cluster where the crime happened\n#we can use the district from the database, but this clusters are more representing since they section the regions according to observed crimes' locations, not as dictated by authorities.\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml.evaluation import ClusteringEvaluator\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":29},{"cell_type":"code","source":["#Generate features:\n#Generate distance from points of interests. e.g: police station, banks, casinos, pubs, clubs, business\n#list of types of places in google maps API can be found here: https://developers.google.com/places/web-service/supported_types\n\ndef get_nearby_poi(apiKey, Xcoordinate, Ycoordinate, listOfInterests, radius):\n    \"\"\"\n    Returns a list of places IDs near the passed location. The functions look for a list of particular interests (listOfInterests), e.g. [church, bank, petrol station, bar, etc...]\n    API: https://developers.google.com/maps/documentation/geocoding/start\n\n    # INPUT \n    apiKey                  [str]\n    location                 [str]\n    listOfInterests          list of strings\n    \n    # RETURN\n    places                   list of place IDs; unique google IDs to identify a place \n    \"\"\"\n    import requests\n    places=[]\n    for i in range(len(listOfInterests)):\n      url = ('https://maps.googleapis.com/maps/api/place/nearbysearch/json?location={},{}&type={}&rankby=distance&key={}'\n             .format(Xcoordinate, Ycoordinate, listOfInterests[i], apiKey))\n      try:\n          output = requests.get(url).json()\n          places.append(output['results'][0]['name']) #get only the first place in the list of interesting places., you can get place_id which is confirmed to be unique.\n\n      except:\n          print('ERROR: cannot query {}, {}'.format(Xcoordinate, Ycoordinate))\n    return places\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":30},{"cell_type":"code","source":["#define global parameters\nlistOfInterests=['bank', 'bar', 'liquor_store'] #night_club, mosque, church, stadium, shopping_mall, police\nradius=5000 #search big area (5K diameter), this version won't use radius as it uses rankby=distance instead to return the list sorted with distance\napiKey='AIzaSyA7GdIrU2iY6pD7fda6PVO1ntE-zmz2C0Q'\n\n#test the function\nyy=data.select('X', 'Y').first()[0]\nxx=data.select('X', 'Y').first()[1]\np=get_nearby_poi(apiKey, xx, yy, listOfInterests, radius)\nprint(p)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&#39;ATM Walgreens 13668&#39;, &#39;The Riddler&#39;, &#39;Oak Gourmet (OakHill Market)&#39;]\n</div>"]}}],"execution_count":31},{"cell_type":"code","source":["UniqueXY = data.select('X', 'Y').distinct()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":32},{"cell_type":"code","source":["from pyspark.sql.types import ArrayType\nPointsOfInterest_udf_ArrayString= udf(lambda arr: get_nearby_poi(apiKey, arr[1], arr[0], listOfInterests, radius), ArrayType(StringType()))\nsample = UniqueXY.limit(2)\n#sample.show()\nsample=sample.withColumn(\"POI\", PointsOfInterest_udf_ArrayString(array(col('X'), col('Y'))))\nfor i in range(len(listOfInterests)):\n  sample=sample.withColumn(listOfInterests[i],col('POI')[i])\nsample = sample.drop(\"POI\")\nsample.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"dbfs:/FileStore/df/BD2_PROJECT_POI_FROM_GOOGLE_API.csv\") #saving csv on databricks.\n#to download, follow instructions here: https://docs.databricks.com/data/filestore.html"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":33},{"cell_type":"code","source":["#Updata num_var list of numerical variables in the dataframe to include the newly generated features.\nnum_var = [i[0] for i in data.dtypes if ((i[1]=='int') | (i[1]=='double')) ]"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["#Normalization, standardization of numerical features.\n\n#1. get mean and stddev for each of the numerical features and then scale the features to standardize all to mean of 0 and stddev of 1. \nfrom pyspark.sql.functions import mean, stddev\ndata_stats={num_var[counter]:([data.select(mean(c)).first()[0], data.select(stddev(c)).first()[0]]) for counter, c in enumerate(data[num_var])}\nfor i in range(len(num_var)):\n  data=data.withColumn(num_var[i], (data[num_var[i]]-data_stats.get(num_var[i])[0])/data_stats.get(num_var[i])[1])\n  \n\n\n\n\n  "],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["#The next part is for steps of the pipeline:\n#1.encoding categorical features.\n#2.enconding labels\n#3.Vector assembler\n#4.Model(s)\n#5.evaluation(s)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["#PCA:\nfrom pyspark.ml.feature import PCA\nfrom pyspark.ml.feature import  VectorAssembler\nfrom pyspark.ml import Pipeline\n\nassemblerForPCA = VectorAssembler(inputCols = num_var, outputCol = \"features\")\npca = PCA(k=3, inputCol=\"features\", outputCol=\"PCAFeatures\")\n\nPCApipeline = Pipeline(stages =[assemblerForPCA , pca])\n\ndataPCA = PCApipeline.fit(data).transform(data)\ndataPCA.printSchema()\n\n#unpack the PCA feature vector to different columns and drop the old features column. \n#code here."],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["encoding_var = [i[0] for i in data.dtypes if (i[1]=='string')& (i[0]!='Category')] #where Category is the label/target\n#encoding_var = [i[0] for i in crime_df.dtypes if (i[1]=='string')& (i[0]!='Category') ]#where category is the label/target\n\nprint(encoding_var)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["#apply StringIndexer() to assign indices to each category in our categorical columns.\nfrom pyspark.ml.feature import StringIndexer\nstring_indexes = [StringIndexer(inputCol = c, outputCol = 'IDX_' + c, handleInvalid = 'keep') for c in encoding_var]\nstring_indexes"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["#ONE Hot Encoding \nfrom pyspark.ml.feature import OneHotEncoderEstimator\nonehot_indexes = [OneHotEncoderEstimator(inputCols = ['IDX_' + c], outputCols = ['OHE_' + c]) for c in encoding_var]\nonehot_indexes"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["label_indexes = StringIndexer(inputCol = 'Category', outputCol = 'label', handleInvalid = 'keep')"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["#the below section is to specify the pipeline"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["assembler = VectorAssembler(inputCols = num_var + ['OHE_' + c for c in encoding_var], outputCol = \"features\")"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["from pyspark.ml.classification import  RandomForestClassifier\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", seed = 8464,\n                            numTrees=10, cacheNodeIds = True, subsamplingRate = 0.7)"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["from pyspark.ml import Pipeline\npipeline = Pipeline(stages = string_indexes + onehot_indexes + [assembler,label_indexes, rf])pipeline = Pipeline(stages = string_indexes + onehot_indexes + [assembler,label_indexes, rf])"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["# model the "],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["fit the data:\n\npipelineModel = pipeline.fit(data)"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["#transform:\nnew_df = pipelineModel.transform(data)\nvhouse_df = new_df.select(['features', 'label'])\nvhouse_df.show()"],"metadata":{},"outputs":[],"execution_count":48}],"metadata":{"name":"CrimesFeatureManipulations","notebookId":3924998272563575},"nbformat":4,"nbformat_minor":0}
