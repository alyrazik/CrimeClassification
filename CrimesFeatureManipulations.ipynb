{"cells":[{"cell_type":"code","source":["#INPUT: train.csv\n#Output: \n\"\"\"\n\n1. Clean up wrong X and Y values (very few of them)\n\n2. visualize data.\n\n2. Parse input to get features: for e.g: get date, time, year, month, etc..)\n\n3. Select, and generate features.\n\n3. Remove outliers.\n\n4. do PCA\n\nOutput: train dataframe with features and labels column\n\n        test dataframe with features and lables column\n\n        visuals to provide insights on data that help select, and tune the models.       \n\n a toolbox list to choose from:\n\n         Typical graphical techniques used in EDA are\n\n\nBox plot\n\nHistogram\n\nMulti-vari chart\n\nRun chart\n\nPareto chart\n\nScatter plot\n\nStem-and-leaf plot\n\nParallel coordinates\n\nOdds ratio\n\nTargeted projection pursuit\n\nGlyph-based visualization methods such as PhenoPlot[8] and Chernoff faces\n\nProjection methods such as grand tour, guided tour and manual tour\n\nInteractive versions of these plots\n\n        Dimensionality reduction:\n\nMultidimensional scaling\n\nPrincipal component analysis (PCA)\n\nMultilinear PCA\n\nNonlinear dimensionality reduction (NLDR)\n\n        Typical quantitative techniques are:\nMedian polish\n\nTrimean\n\nOrdination\n\nHistory\n\n        \n\"\"\"\n\"\"\"\nDates - timestamp of the crime incident\nCategory - category of the crime incident (only in train.csv). This is the target variable you are going to predict.\nDescript - detailed description of the crime incident (only in train.csv)\nDayOfWeek - the day of the week\nPdDistrict - name of the Police Department District\nResolution - how the crime incident was resolved (only in train.csv)\nAddress - the approximate street address of the crime incident \nX - Longitude\nY - Latitude\n\"\"\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[2]: &#39;\\nDates - timestamp of the crime incident\\nCategory - category of the crime incident (only in train.csv). This is the target variable you are going to predict.\\nDescript - detailed description of the crime incident (only in train.csv)\\nDayOfWeek - the day of the week\\nPdDistrict - name of the Police Department District\\nResolution - how the crime incident was resolved (only in train.csv)\\nAddress - the approximate street address of the crime incident \\nX - Longitude\\nY - Latitude\\n&#39;</div>"]}}],"execution_count":1},{"cell_type":"code","source":["filename=\"/FileStore/tables/train.csv\"\ndata=spark.read.csv(filename, header=True, inferSchema=True)\nprint(data.count())\nprint(len(data.columns))\ndata.printSchema() #the data was inferred properly. Class is an int. Features are double."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">878049\n9\nroot\n-- Dates: timestamp (nullable = true)\n-- Category: string (nullable = true)\n-- Descript: string (nullable = true)\n-- DayOfWeek: string (nullable = true)\n-- PdDistrict: string (nullable = true)\n-- Resolution: string (nullable = true)\n-- Address: string (nullable = true)\n-- X: double (nullable = true)\n-- Y: double (nullable = true)\n\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["data = data.drop('Descript').drop('Resolution') # we drop both as they are not available in test data. "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["#1. Data Wrangling to audit the quality of the data and perform all the necessary actions to clean the dataset.\n#1- check how many categorical and numerical features we have\ncat_cols = [item[0] for item in data.dtypes if (item[1]=='string') & (item[0]!='Category')]\nprint(str(len(cat_cols)) + '  categorical features')\nnum_var = [i[0] for i in data.dtypes if ((i[1]=='int') | (i[1]=='double')) ]\nprint(str(len(num_var)) + '  numerical features')\n#Last feature is timestamp"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">3  categorical features\n2  numerical features\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["#class count (labels)\nprint('Number of labels is', data.groupBy('Category').count().count())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Number of labels is 39\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["labels=data[['Category']].distinct().toPandas().values.tolist()\nlabels #we shall use it later.\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[117]: [[&#39;FRAUD&#39;],\n [&#39;SUICIDE&#39;],\n [&#39;SEX OFFENSES FORCIBLE&#39;],\n [&#39;LIQUOR LAWS&#39;],\n [&#39;SECONDARY CODES&#39;],\n [&#39;FAMILY OFFENSES&#39;],\n [&#39;MISSING PERSON&#39;],\n [&#39;OTHER OFFENSES&#39;],\n [&#39;DRIVING UNDER THE INFLUENCE&#39;],\n [&#39;WARRANTS&#39;],\n [&#39;ARSON&#39;],\n [&#39;SEX OFFENSES NON FORCIBLE&#39;],\n [&#39;FORGERY/COUNTERFEITING&#39;],\n [&#39;GAMBLING&#39;],\n [&#39;BRIBERY&#39;],\n [&#39;ASSAULT&#39;],\n [&#39;DRUNKENNESS&#39;],\n [&#39;EXTORTION&#39;],\n [&#39;TREA&#39;],\n [&#39;WEAPON LAWS&#39;],\n [&#39;LOITERING&#39;],\n [&#39;SUSPICIOUS OCC&#39;],\n [&#39;ROBBERY&#39;],\n [&#39;PROSTITUTION&#39;],\n [&#39;EMBEZZLEMENT&#39;],\n [&#39;BAD CHECKS&#39;],\n [&#39;DISORDERLY CONDUCT&#39;],\n [&#39;RUNAWAY&#39;],\n [&#39;RECOVERED VEHICLE&#39;],\n [&#39;VANDALISM&#39;],\n [&#39;DRUG/NARCOTIC&#39;],\n [&#39;PORNOGRAPHY/OBSCENE MAT&#39;],\n [&#39;TRESPASS&#39;],\n [&#39;VEHICLE THEFT&#39;],\n [&#39;NON-CRIMINAL&#39;],\n [&#39;STOLEN PROPERTY&#39;],\n [&#39;LARCENY/THEFT&#39;],\n [&#39;KIDNAPPING&#39;],\n [&#39;BURGLARY&#39;]]</div>"]}}],"execution_count":6},{"cell_type":"code","source":["#check for nulls\nfrom pyspark.sql.functions import isnan, when, count, col\ndata.select([count(when( col(c).isNull(), c)).alias(c) for c in data.columns]).show()\n#conclusion :From above it seems the data is clean with no missing values"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+--------+---------+----------+-------+---+---+\nDates|Category|DayOfWeek|PdDistrict|Address|  X|  Y|\n+-----+--------+---------+----------+-------+---+---+\n    0|       0|        0|         0|      0|  0|  0|\n+-----+--------+---------+----------+-------+---+---+\n\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["#check for data imbalance (classes imbalance)\n#report distinct classes and their prior probabilities as in the dataset.\n\nclassList = data.select('Category').groupBy('Category').agg((100*count('Category')/data.count()).alias('prior'))\nprint(classList.toPandas())\n\n#consider removing 'Other offences', note that it accounts for 14%"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/python/lib/python3.7/site-packages/pyarrow/__init__.py:152: UserWarning: pyarrow.open_stream is deprecated, please use pyarrow.ipc.open_stream\n  warnings.warn(&#34;pyarrow.open_stream is deprecated, please use &#34;\n                       Category      prior\n0                         FRAUD   1.899552\n1                       SUICIDE   0.057856\n2         SEX OFFENSES FORCIBLE   0.499744\n3                   LIQUOR LAWS   0.216731\n4               SECONDARY CODES   1.137180\n5               FAMILY OFFENSES   0.055919\n6                MISSING PERSON   2.959858\n7                OTHER OFFENSES  14.370724\n8   DRIVING UNDER THE INFLUENCE   0.258300\n9                      WARRANTS   4.807704\n10                        ARSON   0.172314\n11    SEX OFFENSES NON FORCIBLE   0.016856\n12       FORGERY/COUNTERFEITING   1.208247\n13                     GAMBLING   0.016628\n14                      BRIBERY   0.032914\n15                      ASSAULT   8.755320\n16                  DRUNKENNESS   0.487444\n17                    EXTORTION   0.029156\n18                         TREA   0.000683\n19                  WEAPON LAWS   0.974319\n20                    LOITERING   0.139514\n21               SUSPICIOUS OCC   3.577705\n22                      ROBBERY   2.619444\n23                 PROSTITUTION   0.852344\n24                 EMBEZZLEMENT   0.132794\n25                   BAD CHECKS   0.046239\n26           DISORDERLY CONDUCT   0.492000\n27                      RUNAWAY   0.221628\n28            RECOVERED VEHICLE   0.357383\n29                    VANDALISM   5.093679\n30                DRUG/NARCOTIC   6.146696\n31      PORNOGRAPHY/OBSCENE MAT   0.002506\n32                     TRESPASS   0.834350\n33                VEHICLE THEFT   6.125057\n34                 NON-CRIMINAL  10.512397\n35              STOLEN PROPERTY   0.517055\n36                LARCENY/THEFT  19.919162\n37                   KIDNAPPING   0.266614\n38                     BURGLARY   4.185985\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["#explore categorical features\n#check # of unique number of categories for categorical features.\nif 'Category' not in cat_cols:\n  cat_cols.append('Category') #add category to cat_cols to view unique values of labels and for the remaining steps\ncountUniqueValues = [data.select(c).distinct().count() for c in cat_cols]\nprint(cat_cols)\nprint(countUniqueValues)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&#39;DayOfWeek&#39;, &#39;PdDistrict&#39;, &#39;Address&#39;, &#39;Category&#39;]\n[7, 10, 23228, 39]\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["#identify the most frequent items in the categorical features.\nfrom pyspark.sql.functions import desc\nN=10 #the N number of most frequent points.\nfrequentCategories = [ data.groupBy(c).agg((100*count(c)/data.count()).alias('Percentage')).sort(desc('Percentage')).limit(N) for c in cat_cols]\nfor i in range(len(cat_cols)):\n  print(frequentCategories[i].toPandas())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">   DayOfWeek  Percentage\n0     Friday   15.230813\n1  Wednesday   14.715694\n  PdDistrict  Percentage\n0   SOUTHERN   17.901279\n1    MISSION   13.656185\n                  Address  Percentage\n0  800 Block of BRYANT ST    3.021813\n1  800 Block of MARKET ST    0.749503\n         Category  Percentage\n0   LARCENY/THEFT   19.919162\n1  OTHER OFFENSES   14.370724\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["#check correlations of features with label column (crime category)\nN=10 #the N number of most frequent pairs of features and class.\nif 'Category' in cat_cols:\n  cat_cols.remove('Category') #remove Category from cat_cols to validate syntax in the loop\nfrequentCategories = [data.groupBy(c, 'Category').agg((100*count(c)/data.count()).alias('PairFrequencyPercentage')).sort(desc('PairFrequencyPercentage')).limit(N) for c in cat_cols]\nfor i in range(len(cat_cols)):\n  print(frequentCategories[i].toPandas())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">   DayOfWeek        Category  PairFrequencyPercentage\n0   Saturday   LARCENY/THEFT                 3.099713\n1     Friday   LARCENY/THEFT                 3.086844\n2  Wednesday   LARCENY/THEFT                 2.788797\n3   Thursday   LARCENY/THEFT                 2.780597\n4     Sunday   LARCENY/THEFT                 2.750416\n5    Tuesday   LARCENY/THEFT                 2.728435\n6     Monday   LARCENY/THEFT                 2.684360\n7  Wednesday  OTHER OFFENSES                 2.272083\n8    Tuesday  OTHER OFFENSES                 2.142136\n9     Friday  OTHER OFFENSES                 2.116966\n   PdDistrict        Category  PairFrequencyPercentage\n0    SOUTHERN   LARCENY/THEFT                 4.765679\n1    NORTHERN   LARCENY/THEFT                 3.260638\n2     CENTRAL   LARCENY/THEFT                 2.854055\n3    SOUTHERN  OTHER OFFENSES                 2.426744\n4    SOUTHERN    NON-CRIMINAL                 2.248736\n5     MISSION  OTHER OFFENSES                 2.201472\n6     MISSION   LARCENY/THEFT                 2.075397\n7  TENDERLOIN   DRUG/NARCOTIC                 2.015377\n8     BAYVIEW  OTHER OFFENSES                 1.942147\n9  TENDERLOIN  OTHER OFFENSES                 1.563011\n                    Address        Category  PairFrequencyPercentage\n0    800 Block of BRYANT ST   LARCENY/THEFT                 0.699733\n1    800 Block of BRYANT ST    NON-CRIMINAL                 0.635842\n2    800 Block of MARKET ST   LARCENY/THEFT                 0.370822\n3    800 Block of BRYANT ST  OTHER OFFENSES                 0.343830\n4    800 Block of BRYANT ST         ASSAULT                 0.219350\n5  2000 Block of MISSION ST   DRUG/NARCOTIC                 0.212517\n6    800 Block of BRYANT ST        WARRANTS                 0.195775\n7   1400 Block of PHELPS ST  MISSING PERSON                 0.167189\n8  2000 Block of MISSION ST  OTHER OFFENSES                 0.137692\n9    800 Block of BRYANT ST       VANDALISM                 0.131542\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["#Explore numerical features. "],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["countUniqueValuesN = [data.select(c).distinct().count() for c in num_var]\nprint(num_var)\nprint(countUniqueValuesN)\n#X, Y repeat.. possibly same locations witness several crimes over and over!"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&#39;X&#39;, &#39;Y&#39;]\n[34243, 34243]\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["#to confirm that X repeats when Y repeat\nNcrimeLocations = data.groupBy('X', 'Y').agg(count('X')).count()\nprint(NcrimeLocations) #shows only 34243 unique pairs"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">34243\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["#we will need to visualize the data\nimport matplotlib.pyplot as plt\nimport pandas as pd\nN1=data.count() if data.count()<=1000000 else 1000000"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":15},{"cell_type":"code","source":["#plot a historgram of timestamp to check if a pattern is there.\nimport numpy as np\ndata_pd_timestamp = data[['Dates']].limit(N1).toPandas()\nplt.clf()\nplt.hist(np.array(data_pd_timestamp['Dates']), bins = 100)\ndisplay(plt.show())"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# check for outliers\n#check in XY spatial data to spot outliers. \n#visualize data to spot outliers.\n\ndata_pd = data[['X', 'Y']].limit(N1).toPandas()\nplt.clf()\ndata_pd.plot(kind=\"scatter\", x=\"X\", y=\"Y\")\ndisplay(plt.show())"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["#Outliers can be seen. Dropping them."],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["outliersCount=data.count()\ndata=data.where('X<-122')\noutliersCount-=data.count()\ndata_pd=data.toPandas()\nplt.clf()\ndata_pd.plot(kind=\"scatter\", x=\"X\", y=\"Y\")\ndisplay(plt.show())"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["print('Removing ', str(outliersCount), ' outliers in spatial data.') #67 cases."],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["#plot XY data for different categories. \nNX=100\nNY=100\n\ngroups = [(data.where(col('Category')==crime[0]).toPandas(), crime[0])  for crime in labels]\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":21},{"cell_type":"code","source":["print(len(crimeRegionsHistogram.T))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">100\n</div>"]}}],"execution_count":22},{"cell_type":"code","source":["#histogram values of x and y into bins and dipslay the bins content using imshow(). the light colors indicate high number of incidences. \nNX=70 #I find 70 to give good visualization. \nNY=70\ni=1\nfig= plt.figure(figsize=(30, 30)) #width and height in inches.\nfor g in groups:\n    group=g[0]\n    name=g[1]\n    plt.subplot(8,5,i)\n    crimeRegionsHistogram, xedges, yedges = np.histogram2d(np.array(group.X),np.array(group.Y), bins=(NX,NY)) # [int, int], the number of bins in each dimension (nx, ny = bins).\n    #xedges now contains the bin boundaries along x and so is yedges along y, the first and last boundary define the region (interval)\n    histoExtent  = [xedges[0],xedges[-1],yedges[0],yedges[-1]]\n    plt.imshow(crimeRegionsHistogram.T, extent=histoExtent, aspect='auto', origin='low') #the points are automatically scaled linearly mapping the lowest value to 0 and the highest to 1. \n    #plt.imshow(crimeRegionsHistogram.T,origin='low',extent=histoExtent,interpolation='nearest',aspect='auto') #transpose so each row list bins with common y range.\n    plt.title(name)\n    i+=1\ndisplay(plt.show())\n# we can tell how important is location features especially for some categories of crimes where the crime category is so dependent on location."],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["#the following code only plots the scatter plot of XY for different categories. You can tell that some crimes take place only in a short list of places. \ni=0\nj=0\nfig, axs = plt.subplots(8, 5, figsize=(30, 30))\nfor g in groups:\n    group=g[0]\n    name=g[1]\n    axs[i,j].scatter(group.X, group.Y)\n    axs[i,j].title.set_text(name)\n    if j < 4:\n      j+=1\n    else:\n      j=0\n      i+=1\n    \ndisplay(plt.show())"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["#SECOND BLOCK. PARSING AND FEATURE GENERATION."],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["#Parsing the time column to generate features, year, month, day, hour, season\nfrom pyspark.sql.functions import col, hour, minute, second, year, month, dayofmonth, date_format\n\ndef season(month):\n  switcher={\n    1:'winter',\n    2:'winter',\n    3:'spring',\n    4:'spring',\n    5:'spring',\n    6:'summer',\n    7:'summer',\n    8:'summer',\n    9:'autumn',\n    10:'autumn',\n    11:'autumn',\n    12:'winter'\n  }\n  return switcher.get(month,\"NA\")\n\nfrom pyspark.sql.types import StringType\nseason_udf_string= udf(lambda x: season(x), StringType())\n\ndata = data.withColumn(\"hour\", hour(col(\"Dates\"))).withColumn(\"minute\", minute(col(\"Dates\"))).withColumn(\"dayOfMonth\", dayofmonth(col(\"Dates\"))).withColumn(\"year\", year(col(\"Dates\"))).withColumn(\"month\", month(col(\"Dates\"))).withColumn(\"weekday\", date_format(col(\"Dates\"), \"EEEE\")).withColumn(\"season\", season_udf_string(col(\"month\"))).drop(col(\"Dates\"))\n\ndata.printSchema()"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["#generate more features here... get either numerical or string valued features. "],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["#Generate features:\n#1\n#define a pipeline to cluster X, Y data and generate a new feature; the cluster in which the crime happened\n#we can use the district from the database, but this clusters are more representing since they section the regions according to observed crimes, not as dictated by authorities.\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml.evaluation import ClusteringEvaluator\n#\n\n#2\n#Generate distance from points of interests. e.g: police station, banks, casinos, pubs, clubs, business"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["#Updata num_var list of numerical variables in the dataframe to include the newly generated features.\nnum_var = [i[0] for i in data.dtypes if ((i[1]=='int') | (i[1]=='double')) ]"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["#Normalization, standardization of numerical features.\n\n#1. get mean and stddev for each of the numerical features and then scale the features to standardize all to mean of 0 and stddev of 1. \nfrom pyspark.sql.functions import mean, stddev\ndata_stats={num_var[counter]:([data.select(mean(c)).first()[0], data.select(stddev(c)).first()[0]]) for counter, c in enumerate(data[num_var])}\nfor i in range(len(num_var)):\n  data=data.withColumn(num_var[i], (data[num_var[i]]-data_stats.get(num_var[i])[0])/data_stats.get(num_var[i])[1])\n  \n\n\n\n\n  "],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["#The next part is for steps of the pipeline:\n#1.encoding categorical features.\n#2.enconding labels\n#3.Vector assembler\n#4.Model(s)\n#5.evaluation(s)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["#PCA:\nfrom pyspark.ml.feature import PCA\nfrom pyspark.ml.feature import  VectorAssembler\nfrom pyspark.ml import Pipeline\n\nassemblerForPCA = VectorAssembler(inputCols = num_var, outputCol = \"features\")\npca = PCA(k=3, inputCol=\"features\", outputCol=\"PCAFeatures\")\n\nPCApipeline = Pipeline(stages =[assemblerForPCA , pca])\n\ndataPCA = PCApipeline.fit(data).transform(data)\ndataPCA.printSchema()\n\n#unpack the PCA feature vector to different columns and drop the old features column. \n#code here."],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["encoding_var = [i[0] for i in data.dtypes if (i[1]=='string')& (i[0]!='Category')] #where Category is the label/target\n#encoding_var = [i[0] for i in crime_df.dtypes if (i[1]=='string')& (i[0]!='Category') ]#where category is the label/target\n\nprint(encoding_var)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["#apply StringIndexer() to assign indices to each category in our categorical columns.\nfrom pyspark.ml.feature import StringIndexer\nstring_indexes = [StringIndexer(inputCol = c, outputCol = 'IDX_' + c, handleInvalid = 'keep') for c in encoding_var]\nstring_indexes"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["#ONE Hot Encoding \nfrom pyspark.ml.feature import OneHotEncoderEstimator\nonehot_indexes = [OneHotEncoderEstimator(inputCols = ['IDX_' + c], outputCols = ['OHE_' + c]) for c in encoding_var]\nonehot_indexes"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["label_indexes = StringIndexer(inputCol = 'Category', outputCol = 'label', handleInvalid = 'keep')"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["#the below section is to specify the pipeline"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["assembler = VectorAssembler(inputCols = num_var + ['OHE_' + c for c in encoding_var], outputCol = \"features\")"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["from pyspark.ml.classification import  RandomForestClassifier\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", seed = 8464,\n                            numTrees=10, cacheNodeIds = True, subsamplingRate = 0.7)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["from pyspark.ml import Pipeline\npipeline = Pipeline(stages = string_indexes + onehot_indexes + [assembler,label_indexes, rf])pipeline = Pipeline(stages = string_indexes + onehot_indexes + [assembler,label_indexes, rf])"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["# model the "],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["fit the data:\n\npipelineModel = pipeline.fit(data)"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["#transform:\nnew_df = pipelineModel.transform(data)\nvhouse_df = new_df.select(['features', 'label'])\nvhouse_df.show()"],"metadata":{},"outputs":[],"execution_count":43}],"metadata":{"name":"CrimesFeatureManipulations","notebookId":3924998272563575},"nbformat":4,"nbformat_minor":0}
