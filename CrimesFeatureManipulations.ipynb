{"cells":[{"cell_type":"code","source":["#INPUT: train.csv\n#Output: \n\"\"\"\n\n1. Clean up wrong X and Y values (very few of them)\n\n2. visualize data.\n\n2. Parse input to get features: for e.g: get date, time, year, month, etc..)\n\n3. Select, and generate features.\n\n3. Remove outliers.\n\n4. do PCA\n\nOutput: train dataframe with features and labels column\n\n        test dataframe with features and lables column\n\n        visuals to provide insights on data that help select, and tune the models.       \n\n a toolbox list to choose from:\n\n         Typical graphical techniques used in EDA are\n\n\nBox plot\n\nHistogram\n\nMulti-vari chart\n\nRun chart\n\nPareto chart\n\nScatter plot\n\nStem-and-leaf plot\n\nParallel coordinates\n\nOdds ratio\n\nTargeted projection pursuit\n\nGlyph-based visualization methods such as PhenoPlot[8] and Chernoff faces\n\nProjection methods such as grand tour, guided tour and manual tour\n\nInteractive versions of these plots\n\n        Dimensionality reduction:\n\nMultidimensional scaling\n\nPrincipal component analysis (PCA)\n\nMultilinear PCA\n\nNonlinear dimensionality reduction (NLDR)\n\n        Typical quantitative techniques are:\nMedian polish\n\nTrimean\n\nOrdination\n\nHistory\n\n        \n\"\"\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[1]: &#39;\\n\\n1. Clean up wrong X and Y values (very few of them)\\n\\n2. visualize data.\\n\\n2. Parse input to get features: for e.g: get date, time, year, month, etc..)\\n\\n3. Select, and generate features.\\n\\n3. Remove outliers.\\n\\n4. do PCA\\n\\nOutput: train dataframe with features and labels column\\n\\n        test dataframe with features and lables column\\n\\n        visuals to provide insights on data that help select, and tune the models.       \\n\\n a toolbox list to choose from:\\n\\n         Typical graphical techniques used in EDA are\\n\\n\\nBox plot\\n\\nHistogram\\n\\nMulti-vari chart\\n\\nRun chart\\n\\nPareto chart\\n\\nScatter plot\\n\\nStem-and-leaf plot\\n\\nParallel coordinates\\n\\nOdds ratio\\n\\nTargeted projection pursuit\\n\\nGlyph-based visualization methods such as PhenoPlot[8] and Chernoff faces\\n\\nProjection methods such as grand tour, guided tour and manual tour\\n\\nInteractive versions of these plots\\n\\n        Dimensionality reduction:\\n\\nMultidimensional scaling\\n\\nPrincipal component analysis (PCA)\\n\\nMultilinear PCA\\n\\nNonlinear dimensionality reduction (NLDR)\\n\\n        Typical quantitative techniques are:\\nMedian polish\\n\\nTrimean\\n\\nOrdination\\n\\nHistory\\n\\n        \\n&#39;</div>"]}}],"execution_count":1},{"cell_type":"code","source":["filename=\"/FileStore/tables/train.csv\"\ndata=spark.read.csv(filename, header=True, inferSchema=True)\nprint(data.count())\nprint(len(data.columns))\ndata.printSchema() #the data was inferred properly. Class is an int. Features are double."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">878049\n9\nroot\n-- Dates: timestamp (nullable = true)\n-- Category: string (nullable = true)\n-- Descript: string (nullable = true)\n-- DayOfWeek: string (nullable = true)\n-- PdDistrict: string (nullable = true)\n-- Resolution: string (nullable = true)\n-- Address: string (nullable = true)\n-- X: double (nullable = true)\n-- Y: double (nullable = true)\n\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["\"\"\"\nDates - timestamp of the crime incident\nCategory - category of the crime incident (only in train.csv). This is the target variable you are going to predict.\nDescript - detailed description of the crime incident (only in train.csv)\nDayOfWeek - the day of the week\nPdDistrict - name of the Police Department District\nResolution - how the crime incident was resolved (only in train.csv)\nAddress - the approximate street address of the crime incident \nX - Longitude\nY - Latitude\n\"\"\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[3]: &#39;\\nDates - timestamp of the crime incident\\nCategory - category of the crime incident (only in train.csv). This is the target variable you are going to predict.\\nDescript - detailed description of the crime incident (only in train.csv)\\nDayOfWeek - the day of the week\\nPdDistrict - name of the Police Department District\\nResolution - how the crime incident was resolved (only in train.csv)\\nAddress - the approximate street address of the crime incident \\nX - Longitude\\nY - Latitude\\n&#39;</div>"]}}],"execution_count":3},{"cell_type":"code","source":["#Parsing the time column to generate features, year, month, day, hour, season\nfrom pyspark.sql.functions import col, hour, minute, second, year, month, dayofmonth, date_format\n\ndef season(month):\n  switcher={\n    1:'winter',\n    2:'winter',\n    3:'spring',\n    4:'spring',\n    5:'spring',\n    6:'summer',\n    7:'summer',\n    8:'summer',\n    9:'autumn',\n    10:'autumn',\n    11:'autumn',\n    12:'winter'\n  }\n  return switcher.get(month,\"NA\")\n\nfrom pyspark.sql.types import StringType\nseason_udf_string= udf(lambda x: season(x), StringType())\n\ndata = data.withColumn(\"hour\", hour(col(\"Dates\"))).withColumn(\"minute\", minute(col(\"Dates\"))).withColumn(\"dayOfMonth\", dayofmonth(col(\"Dates\"))).withColumn(\"year\", year(col(\"Dates\"))).withColumn(\"month\", month(col(\"Dates\"))).withColumn(\"weekday\", date_format(col(\"Dates\"), \"EEEE\")).withColumn(\"season\", season_udf_string(col(\"month\"))).drop(col(\"Dates\"))\n\ndata.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- Category: string (nullable = true)\n-- Descript: string (nullable = true)\n-- DayOfWeek: string (nullable = true)\n-- PdDistrict: string (nullable = true)\n-- Resolution: string (nullable = true)\n-- Address: string (nullable = true)\n-- X: double (nullable = true)\n-- Y: double (nullable = true)\n-- hour: integer (nullable = true)\n-- minute: integer (nullable = true)\n-- dayOfMonth: integer (nullable = true)\n-- year: integer (nullable = true)\n-- month: integer (nullable = true)\n-- weekday: string (nullable = true)\n-- season: string (nullable = true)\n\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["#generate more features here... get either numerical or string valued features. "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["#1. Data Wrangling to audit the quality of the data and perform all the necessary actions to clean the dataset.\n#1- check how many categorical and numerical features we have\ncat_cols = [item[0] for item in data.dtypes if item[1].startswith('string')] \nprint(str(len(cat_cols)) + '  categorical features')\n\nnum_var = [i[0] for i in data.dtypes if ((i[1]=='int') | (i[1]=='double')) ]\nprint(str(len(num_var)) + '  numerical features')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">8  categorical features\n7  numerical features\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["#check for nulls\nfrom pyspark.sql.functions import isnan, when, count, col\ndata.select([count(when( col(c).isNull(), c)).alias(c) for c in data.columns]).show()\n#conclusion :From above it seems the data is clean with no missing values"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+--------+---------+----------+----------+-------+---+---+----+------+----------+----+-----+-------+------+\nCategory|Descript|DayOfWeek|PdDistrict|Resolution|Address|  X|  Y|hour|minute|dayOfMonth|year|month|weekday|season|\n+--------+--------+---------+----------+----------+-------+---+---+----+------+----------+----+-----+-------+------+\n       0|       0|        0|         0|         0|      0|  0|  0|   0|     0|         0|   0|    0|      0|     0|\n+--------+--------+---------+----------+----------+-------+---+---+----+------+----------+----+-----+-------+------+\n\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["#Visualizations:\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndata_pd=data.toPandas()\nplt.clf()\ndata_pd.plot(kind=\"scatter\", x=\"X\", y=\"Y\")\ndisplay(plt.show())"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["#we can see an outlier in the dataset, removing it."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["data=data.where('X<-122')\ndata_pd=data.toPandas()\nplt.clf()\ndata_pd.plot(kind=\"scatter\", x=\"X\", y=\"Y\")\ndisplay(plt.show())"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["#More preprocessing to the features:\n\n#1. get mean and stddev for each of the numerical features and then scale the features to standardize all to mean of 0 and stddev of 1.\nfrom pyspark.sql.functions import mean, stddev\ndata_stats={num_var[counter]:([data.select(mean(c)).first()[0], data.select(stddev(c)).first()[0]]) for counter, c in enumerate(data[num_var])}\nfor i in range(len(num_var)):\n  data=data.withColumn(num_var[i], (data[num_var[i]]-data_stats.get(num_var[i])[0])/data_stats.get(num_var[i])[1])\n\n  "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"code","source":["encoding_var = [i[0] for i in data.dtypes if (i[1]=='string')& (i[0]!='Category')] #where Category is the label/target\n#encoding_var = [i[0] for i in crime_df.dtypes if (i[1]=='string')& (i[0]!='Category') ]#where category is the label/target\n\nprint(encoding_var)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&#39;Descript&#39;, &#39;DayOfWeek&#39;, &#39;PdDistrict&#39;, &#39;Resolution&#39;, &#39;Address&#39;, &#39;weekday&#39;, &#39;season&#39;]\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["#apply StringIndexer() to assign indices to each category in our categorical columns.\nfrom pyspark.ml.feature import StringIndexer\nstring_indexes = [StringIndexer(inputCol = c, outputCol = 'IDX_' + c, handleInvalid = 'keep') for c in encoding_var]\nstring_indexes"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[13]: [StringIndexer_0bdd5aaa1fa2,\n StringIndexer_3c3d23f7c939,\n StringIndexer_1bcfdef066cb,\n StringIndexer_38bb84c3e37f,\n StringIndexer_bca94e4d266b,\n StringIndexer_f28e7deb006f,\n StringIndexer_05ba73131a3d]</div>"]}}],"execution_count":14},{"cell_type":"code","source":["#ONE Hot Encoding \nfrom pyspark.ml.feature import OneHotEncoderEstimator\nonehot_indexes = [OneHotEncoderEstimator(inputCols = ['IDX_' + c], outputCols = ['OHE_' + c]) for c in encoding_var]\nonehot_indexes"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[14]: [OneHotEncoderEstimator_f53dca306167,\n OneHotEncoderEstimator_3c27ff812042,\n OneHotEncoderEstimator_5dd334f7f212,\n OneHotEncoderEstimator_6d14be8639ce,\n OneHotEncoderEstimator_b9c3c189702f,\n OneHotEncoderEstimator_1d29b655a687,\n OneHotEncoderEstimator_15c9b068a0f3]</div>"]}}],"execution_count":15},{"cell_type":"code","source":["label_indexes = StringIndexer(inputCol = 'Category', outputCol = 'label', handleInvalid = 'keep')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":16},{"cell_type":"code","source":["from pyspark.ml.feature import  VectorAssembler\nassembler = VectorAssembler(inputCols = num_var + ['OHE_' + c for c in encoding_var], outputCol = \"features\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":17},{"cell_type":"code","source":["from pyspark.ml.classification import  RandomForestClassifier\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", seed = 8464,\n                            numTrees=10, cacheNodeIds = True, subsamplingRate = 0.7)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":18},{"cell_type":"code","source":["from pyspark.ml import Pipeline\npipeline = Pipeline(stages = string_indexes + onehot_indexes + [assembler,label_indexes, rf])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":19},{"cell_type":"code","source":["#fit the data:\n\npipelineModel = pipeline.fit(data)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":20},{"cell_type":"code","source":["#transform:\nnew_df = pipelineModel.transform(data)\nvhouse_df = new_df.select(['features', 'label'])\nvhouse_df.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+-----+\n            features|label|\n+--------------------+-----+\n(24122,[0,1,2,3,4...|  7.0|\n(24122,[0,1,2,3,4...|  1.0|\n(24122,[0,1,2,3,4...|  1.0|\n(24122,[0,1,2,3,4...|  0.0|\n(24122,[0,1,2,3,4...|  0.0|\n(24122,[0,1,2,3,4...|  0.0|\n(24122,[0,1,2,3,4...|  5.0|\n(24122,[0,1,2,3,4...|  5.0|\n(24122,[0,1,2,3,4...|  0.0|\n(24122,[0,1,2,3,4...|  0.0|\n(24122,[0,1,2,3,4...|  0.0|\n(24122,[0,1,2,3,4...|  1.0|\n(24122,[0,1,2,3,4...|  6.0|\n(24122,[0,1,2,3,4...|  0.0|\n(24122,[0,1,2,3,4...|  2.0|\n(24122,[0,1,2,3,4...|  2.0|\n(24122,[0,1,2,3,4...| 11.0|\n(24122,[0,1,2,3,4...|  3.0|\n(24122,[0,1,2,3,4...|  1.0|\n(24122,[0,1,2,3,4...|  2.0|\n+--------------------+-----+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":21}],"metadata":{"name":"CrimesFeatureManipulations","notebookId":3924998272563575},"nbformat":4,"nbformat_minor":0}
